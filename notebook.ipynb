{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from typing import NamedTuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils import data\n",
    "from PIL import Image\n",
    "\n",
    "import model\n",
    "from tensor_transforms import convert_to_coord_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFHQ256Arguments(NamedTuple):\n",
    "    \"\"\"CIPSskip for FFHQ-256\"\"\"\n",
    "    Generator = 'CIPSskip'\n",
    "    output_dir = 'ffhq256_g_ema.pt'\n",
    "    out_path = 'checkpoint'\n",
    "    size = 256\n",
    "    coords_size = 256\n",
    "    fc_dim = 512 \n",
    "    latent = 512\n",
    "    style_dim = 512\n",
    "    n_mlp = 8\n",
    "    activation = None\n",
    "    channel_multiplier = 2\n",
    "    ckpt = os.path.join(out_path, output_dir)\n",
    "    coords_integer_values = False\n",
    "    path=\"checkpoint/ffhq_256_g_ema.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFHQ1024Arguments(NamedTuple):\n",
    "    \"\"\"CIPSskip-progressive for FFHQ-1024\"\"\"\n",
    "    Generator = 'CIPSskip'\n",
    "    output_dir = 'ffhq1024_g_ema.pt'\n",
    "    out_path = 'checkpoint'\n",
    "    size = 256\n",
    "    coords_size = 1024\n",
    "    fc_dim = 512 \n",
    "    latent = 512\n",
    "    style_dim = 512\n",
    "    n_mlp = 8\n",
    "    activation = None\n",
    "    channel_multiplier = 2\n",
    "    ckpt = os.path.join(out_path, output_dir)\n",
    "    coords_integer_values = False\n",
    "    path=\"checkpoint/ffhq1024_g_ema.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Churches256Arguments(NamedTuple):\n",
    "    \"\"\"CIPSskip for LSUN-Churches-256\"\"\"\n",
    "    Generator = 'CIPSskip'\n",
    "    output_dir = 'churches_g_ema.pt'\n",
    "    out_path = 'checkpoint'\n",
    "    size = 256\n",
    "    coords_size = 256\n",
    "    fc_dim = 512 \n",
    "    latent = 512\n",
    "    style_dim = 512\n",
    "    n_mlp = 8\n",
    "    activation = None\n",
    "    channel_multiplier = 2\n",
    "    ckpt = os.path.join(out_path, output_dir)\n",
    "    coords_integer_values = False\n",
    "    path=\"checkpoint/churchs_g_ema.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lanscapes256Arguments(NamedTuple):\n",
    "    \"\"\"CIPSres for Landscapes-256\"\"\"\n",
    "    Generator = 'CIPSres'\n",
    "    output_dir = 'landscapes_g_ema.pt'\n",
    "    out_path = 'checkpoint'\n",
    "    size = 256\n",
    "    coords_size = 256\n",
    "    fc_dim = 512 \n",
    "    latent = 512\n",
    "    style_dim = 512\n",
    "    n_mlp = 8\n",
    "    activation = None\n",
    "    channel_multiplier = 2\n",
    "    ckpt = os.path.join(out_path, output_dir)\n",
    "    coords_integer_values = False\n",
    "    path=\"checkpoint/landscapes_g_ema.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=FFHQ256Arguments()\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generator = getattr(model, args.Generator)\n",
    "g_ema = Generator(size=args.size, hidden_size=args.fc_dim, style_dim=args.latent, n_mlp=args.n_mlp,\n",
    "                  activation=args.activation, channel_multiplier=args.channel_multiplier,\n",
    "                  ).to(device)\n",
    "g_ema.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=args.path\n",
    "ckpt = torch.load(path)\n",
    "g_ema.load_state_dict(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(tensor, nrow=2, padding=2,\n",
    "               normalize=False, range=None, scale_each=False, pad_value=0):\n",
    "    \n",
    "    grid = torchvision.utils.make_grid(tensor, nrow=nrow,padding=padding, pad_value=pad_value,\n",
    "                     normalize=normalize, range=range, scale_each=scale_each)\n",
    "    ndarr = grid.mul(255).clamp(0, 255).byte().permute(1, 2, 0).cpu().numpy()\n",
    "    im = Image.fromarray(ndarr)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding mean for truncation trick "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 1\n",
    "sample_z = torch.randn(n_sample, args.latent, device=device)\n",
    "converted_full = convert_to_coord_format(sample_z.size(0), args.coords_size, args.coords_size, device,\n",
    "                                         integer_values=args.coords_integer_values)\n",
    "\n",
    "latents = []\n",
    "samples = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(100):\n",
    "        sample_z = torch.randn(n_sample, args.latent, device=device)\n",
    "        sample, latent = g_ema(converted_full, [sample_z], return_latents=True)\n",
    "        latents.append(latent.cpu())\n",
    "        samples.append(sample.cpu())\n",
    "\n",
    "samples = torch.cat(samples, 0)\n",
    "latents = torch.cat(latents, 0)\n",
    "\n",
    "truncation_latent = latents.mean(0).cuda()\n",
    "\n",
    "print('truncation_latent', truncation_latent.shape)\n",
    "assert len(truncation_latent.shape)==1 and truncation_latent.size(0) == 512, 'smt wrong'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling with truncation trick "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9922, -0.9922, -0.9922,  ..., -0.9922, -0.9922, -0.9922],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.9843, -0.9843, -0.9843],\n",
      "          ...,\n",
      "          [ 0.9843,  0.9843,  0.9843,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [ 0.9922,  0.9922,  0.9922,  ...,  0.9922,  0.9922,  0.9922],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9922, -0.9922, -0.9922,  ..., -0.9922, -0.9922, -0.9922],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.9843, -0.9843, -0.9843],\n",
      "          ...,\n",
      "          [ 0.9843,  0.9843,  0.9843,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [ 0.9922,  0.9922,  0.9922,  ...,  0.9922,  0.9922,  0.9922],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9922, -0.9922, -0.9922,  ..., -0.9922, -0.9922, -0.9922],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.9843, -0.9843, -0.9843],\n",
      "          ...,\n",
      "          [ 0.9843,  0.9843,  0.9843,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [ 0.9922,  0.9922,  0.9922,  ...,  0.9922,  0.9922,  0.9922],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9922, -0.9922, -0.9922,  ..., -0.9922, -0.9922, -0.9922],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.9843, -0.9843, -0.9843],\n",
      "          ...,\n",
      "          [ 0.9843,  0.9843,  0.9843,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [ 0.9922,  0.9922,  0.9922,  ...,  0.9922,  0.9922,  0.9922],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9922, -0.9922, -0.9922,  ..., -0.9922, -0.9922, -0.9922],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.9843, -0.9843, -0.9843],\n",
      "          ...,\n",
      "          [ 0.9843,  0.9843,  0.9843,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [ 0.9922,  0.9922,  0.9922,  ...,  0.9922,  0.9922,  0.9922],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]]],\n",
      "\n",
      "\n",
      "        [[[-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          ...,\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000],\n",
      "          [-1.0000, -0.9922, -0.9843,  ...,  0.9843,  0.9922,  1.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9922, -0.9922, -0.9922,  ..., -0.9922, -0.9922, -0.9922],\n",
      "          [-0.9843, -0.9843, -0.9843,  ..., -0.9843, -0.9843, -0.9843],\n",
      "          ...,\n",
      "          [ 0.9843,  0.9843,  0.9843,  ...,  0.9843,  0.9843,  0.9843],\n",
      "          [ 0.9922,  0.9922,  0.9922,  ...,  0.9922,  0.9922,  0.9922],\n",
      "          [ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000]]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaepoong/anaconda3/envs/jaepoong/lib/python3.9/site-packages/torch/nn/functional.py:4003: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/home/jaepoong/anaconda3/envs/jaepoong/lib/python3.9/site-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.\n",
      "  warnings.warn(warning)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "ERROR:root:dropped chunk 404 Client Error: Not Found for url: https://api.wandb.ai/files/poong2/CIPS/3sdb7cub/file_stream\n",
      "NoneType: None\n"
     ]
    }
   ],
   "source": [
    "n_sample = 8\n",
    "sample_z = torch.randn(n_sample, args.latent, device=device)\n",
    "converted_full = convert_to_coord_format(sample_z.size(0), args.coords_size, args.coords_size, device,\n",
    "                                         integer_values=args.coords_integer_values)\n",
    "    \n",
    "print(converted_full)\n",
    "with torch.no_grad():\n",
    "    style = g_ema.style(sample_z)\n",
    "    print(style.size())\n",
    "    sample, _ = g_ema(converted_full, [style], \n",
    "                      truncation=0.6,\n",
    "                      truncation_latent=truncation_latent,\n",
    "                      input_is_latent=True,)\n",
    "    \n",
    "im = get_image(sample,                        \n",
    "                nrow=4,\n",
    "                normalize=True,\n",
    "                range=(-1, 1),)\n",
    "\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 좌표 변환 실험."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.0000, -0.9980, -0.9961,  ..., -0.0039, -0.0020,  0.0000],\n",
      "          [-1.0000, -0.9980, -0.9961,  ..., -0.0039, -0.0020,  0.0000],\n",
      "          [-1.0000, -0.9980, -0.9961,  ..., -0.0039, -0.0020,  0.0000],\n",
      "          ...,\n",
      "          [-1.0000, -0.9980, -0.9961,  ..., -0.0039, -0.0020,  0.0000],\n",
      "          [-1.0000, -0.9980, -0.9961,  ..., -0.0039, -0.0020,  0.0000],\n",
      "          [-1.0000, -0.9980, -0.9961,  ..., -0.0039, -0.0020,  0.0000]],\n",
      "\n",
      "         [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "          [-0.9980, -0.9980, -0.9980,  ..., -0.9980, -0.9980, -0.9980],\n",
      "          [-0.9961, -0.9961, -0.9961,  ..., -0.9961, -0.9961, -0.9961],\n",
      "          ...,\n",
      "          [-0.0039, -0.0039, -0.0039,  ..., -0.0039, -0.0039, -0.0039],\n",
      "          [-0.0020, -0.0020, -0.0020,  ..., -0.0020, -0.0020, -0.0020],\n",
      "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "ERROR:root:dropped chunk 404 Client Error: Not Found for url: https://api.wandb.ai/files/poong2/CIPS/3sdb7cub/file_stream\n",
      "NoneType: None\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "ERROR:root:dropped chunk 404 Client Error: Not Found for url: https://api.wandb.ai/files/poong2/CIPS/3sdb7cub/file_stream\n",
      "NoneType: None\n"
     ]
    }
   ],
   "source": [
    "n_sample = 1\n",
    "sample_z = torch.randn(n_sample, args.latent, device=device)\n",
    "converted_full = convert_to_coord_format(sample_z.size(0), args.coords_size, args.coords_size, device,\n",
    "                                         integer_values=args.coords_integer_values)\n",
    "\n",
    "    \n",
    "print(converted_full)\n",
    "with torch.no_grad():\n",
    "    style = g_ema.style(sample_z)\n",
    "    print(style.size())\n",
    "    sample, _ = g_ema(converted_full, [style], \n",
    "                      truncation=0.6,\n",
    "                      truncation_latent=truncation_latent,\n",
    "                      input_is_latent=True,)\n",
    "    \n",
    "im = get_image(sample,                        \n",
    "                nrow=int(n_sample ** 0.5),\n",
    "                normalize=True,\n",
    "                range=(-1, 1),)\n",
    "\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3ffdibl0) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 816038... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305e8a152656460f843086686bde9980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 76.78MB of 76.78MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 108 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">FFHQ-256 pretrained generating</strong>: <a href=\"https://wandb.ai/poong2/CIPS/runs/3ffdibl0\" target=\"_blank\">https://wandb.ai/poong2/CIPS/runs/3ffdibl0</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220714_152446-3ffdibl0/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3ffdibl0). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.21 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/poong2/CIPS/runs/1o0qmh5u\" target=\"_blank\">FFHQ-256 pretrained generating</a></strong> to <a href=\"https://wandb.ai/poong2/CIPS\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaepoong/anaconda3/envs/jaepoong/lib/python3.9/site-packages/torch/nn/functional.py:4003: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/home/jaepoong/anaconda3/envs/jaepoong/lib/python3.9/site-packages/torchvision/utils.py:50: UserWarning: range will be deprecated, please use value_range instead.\n",
      "  warnings.warn(warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n",
      "torch.Size([8, 512])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 817750... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dcb3b5fa8074cb6989d35951881960c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 70.66MB of 70.66MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
       "</div><div class=\"wandb-col\">\n",
       "</div></div>\n",
       "Synced 6 W&B file(s), 100 media file(s), 0 artifact file(s) and 0 other file(s)\n",
       "<br/>Synced <strong style=\"color:#cdcd00\">FFHQ-256 pretrained generating</strong>: <a href=\"https://wandb.ai/poong2/CIPS/runs/1o0qmh5u\" target=\"_blank\">https://wandb.ai/poong2/CIPS/runs/1o0qmh5u</a><br/>\n",
       "Find logs at: <code>./wandb/run-20220714_154340-1o0qmh5u/logs</code><br/>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import wandb\n",
    "wandb.init(\n",
    "    project=\"CIPS\",\n",
    "    name=\"FFHQ-256 pretrained generating\",\n",
    ")\n",
    "example=[]\n",
    "n_sample=8\n",
    "for iter in range(100):\n",
    "    sample_z = torch.randn(n_sample, args.latent, device=device)\n",
    "    converted_full = convert_to_coord_format(sample_z.size(0), args.coords_size, args.coords_size, device,\n",
    "                                         integer_values=args.coords_integer_values)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        style = g_ema.style(sample_z)\n",
    "        print(style.size())\n",
    "        sample, _ = g_ema(converted_full, [style], \n",
    "                        truncation=0.6,\n",
    "                        truncation_latent=truncation_latent,\n",
    "                        input_is_latent=True,)\n",
    "        \n",
    "        im = get_image(sample,                        \n",
    "                        nrow=4,\n",
    "                        normalize=True,\n",
    "                        range=(-1, 1),)\n",
    "        image=wandb.Image(im,caption=f\"iter{iter}\")\n",
    "        example.append(image)\n",
    "wandb.log({\"examples\":example})\n",
    "wandb.finish()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'convert_to_dif_coord_format' from 'tensor_transforms' (/home/jaepoong/바탕화면/CIPS/tensor_transforms.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_812240/4012949981.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensor_transforms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconvert_to_dif_coord_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m wandb.init(\n\u001b[1;32m      4\u001b[0m     \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"CIPS\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"FFHQ-256 [-1,0][-1,0]\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'convert_to_dif_coord_format' from 'tensor_transforms' (/home/jaepoong/바탕화면/CIPS/tensor_transforms.py)"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from tensor_transforms import convert_to_dif_coord_format\n",
    "wandb.init(\n",
    "    project=\"CIPS\",\n",
    "    name=\"FFHQ-256 [-1,0][-1,0]\",\n",
    ")\n",
    "example=[]\n",
    "n_sample=8\n",
    "for iter in range(100):\n",
    "    sample_z = torch.randn(n_sample, args.latent, device=device)\n",
    "    converted_full = convert_to_dif_coord_format(sample_z.size(0), args.coords_size, args.coords_size, device,\n",
    "                h_range=(-1.0,0),w_range=(-1.0,0),integer_values=args.coords_integer_values)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        style = g_ema.style(sample_z)\n",
    "        print(style.size())\n",
    "        sample, _ = g_ema(converted_full, [style], \n",
    "                        truncation=0.6,\n",
    "                        truncation_latent=truncation_latent,\n",
    "                        input_is_latent=True,)\n",
    "        \n",
    "        im = get_image(sample,                        \n",
    "                        nrow=4,\n",
    "                        normalize=True,\n",
    "                        range=(-1, 1),)\n",
    "        image=wandb.Image(im,caption=f\"iter{iter}\")\n",
    "        example.append(image)\n",
    "wandb.log({\"examples\":example})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc4bfa6772543d6073f3fd1028986b644e081e8ab7bbf491a79dc89050113a24"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('jaepoong')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
